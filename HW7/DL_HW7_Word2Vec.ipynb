{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxGTjD5G6V0Q",
        "outputId": "ef00a551-1374-471d-c6da-d398cb028415"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGq1XjUcwcWn"
      },
      "outputs": [],
      "source": [
        "# Word2Vec Implementation\n",
        "# Import necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Word2Vec model class\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, target_word, context_word):\n",
        "        target_embed = self.in_embed(target_word)\n",
        "        context_embed = self.out_embed(context_word)\n",
        "        return target_embed, context_embed"
      ],
      "metadata": {
        "id": "ihrIIAXNwgBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, words):\n",
        "        self.vocab = list(set(words))\n",
        "\n",
        "        self.stoi = {v:k for k, v in enumerate(self.vocab)}\n",
        "        self.itos = {k:v for k, v in enumerate(self.vocab)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.stoi)"
      ],
      "metadata": {
        "id": "5UWkV0ikxHO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "\n",
        "def train_word2vec(corpus, window_size, embedding_dim, num_epochs, learning_rate):\n",
        "    # Preprocess the corpus and build the vocabulary\n",
        "    tokens = word_tokenize(corpus)\n",
        "    v = Vocabulary(tokens)\n",
        "\n",
        "    training_pairs = []\n",
        "    # Create the target-context word pairs\n",
        "    for t in range(len(tokens)):\n",
        "        if tokens[t] == '.' or tokens[t] == '!':\n",
        "            continue\n",
        "\n",
        "        for c in range(t-window_size//2, t+1+window_size//2):\n",
        "            if c == t or c < 0 or c >= len(tokens) or tokens[c] == '.' or tokens[c] == '!':\n",
        "                continue\n",
        "\n",
        "            target = tokens[t]\n",
        "            context = tokens[c]\n",
        "            training_pairs.append((torch.tensor(v.stoi[target]), torch.tensor(v.stoi[context])))\n",
        "\n",
        "    # Initialize the Word2Vec model\n",
        "    model = Word2Vec(len(v), embedding_dim)\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        for target_word, context_word in training_pairs:\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass\n",
        "            t, c = model(target_word, context_word)\n",
        "            # Compute the loss\n",
        "            loss = loss_fn(t, c)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update the model parameters\n",
        "            optimizer.step()\n",
        "            # Accumulate the loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Print the average loss for the epoch\n",
        "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(training_pairs):.3f}\")\n",
        "\n",
        "    # Return the trained Word2Vec model and vocab\n",
        "    return (v, model)"
      ],
      "metadata": {
        "id": "DzHH19FowgEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_most_similar(goal, embeddings, stoi, k):\n",
        "\n",
        "    scores = []\n",
        "    goal_embed = embeddings[stoi[goal]]\n",
        "    for (w, idx) in stoi.items():\n",
        "        if w == goal:\n",
        "            continue\n",
        "\n",
        "        s = np.dot(goal_embed, embeddings[idx]) / (np.linalg.norm(goal_embed, 2) * np.linalg.norm(embeddings[idx], 2))\n",
        "        scores.append((w, s))\n",
        "\n",
        "    sort_scores = sorted(scores, key=lambda i: i[1], reverse=True)\n",
        "    return sort_scores[:k]"
      ],
      "metadata": {
        "id": "cVFMyhsMGSDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the main function\n",
        "\n",
        "def main():\n",
        "    # Set hyperparameters\n",
        "    corpus = \"I love to learn deep learning. It is fascinating!\"\n",
        "    window_size = 3\n",
        "    embedding_dim = 10\n",
        "\n",
        "    LR = 1e-2\n",
        "    EPOCHS = 50\n",
        "\n",
        "    # Train the Word2Vec model\n",
        "    vocab, model = train_word2vec(corpus, window_size, embedding_dim, EPOCHS, LR)\n",
        "    embeddings = model.in_embed.weight.detach().numpy()\n",
        "\n",
        "    # Evaluate the trained model using word similarity or analogy tasks\n",
        "    x = k_most_similar(\"learn\", embeddings, vocab.stoi, 3)\n",
        "    print(\"3 most similar words to 'learn':\", x)\n",
        "\n",
        "    x = k_most_similar(\"deep\", embeddings, vocab.stoi, 3)\n",
        "    print(\"3 most similar words to 'deep':\", x)\n",
        "\n",
        "    print()\n",
        "    # Print the learned word embeddings\n",
        "    for (w, idx) in vocab.stoi.items():\n",
        "        print(w, embeddings[idx])\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save(model.state_dict(), './word2vec')\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0MuMxPBQwgJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db764222-dabd-49cc-a8be-05340dc3a03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: -3.915\n",
            "Epoch 2 Loss: -5.597\n",
            "Epoch 3 Loss: -7.172\n",
            "Epoch 4 Loss: -8.753\n",
            "Epoch 5 Loss: -10.352\n",
            "Epoch 6 Loss: -11.975\n",
            "Epoch 7 Loss: -13.629\n",
            "Epoch 8 Loss: -15.318\n",
            "Epoch 9 Loss: -17.049\n",
            "Epoch 10 Loss: -18.828\n",
            "Epoch 11 Loss: -20.661\n",
            "Epoch 12 Loss: -22.554\n",
            "Epoch 13 Loss: -24.515\n",
            "Epoch 14 Loss: -26.549\n",
            "Epoch 15 Loss: -28.662\n",
            "Epoch 16 Loss: -30.861\n",
            "Epoch 17 Loss: -33.151\n",
            "Epoch 18 Loss: -35.539\n",
            "Epoch 19 Loss: -38.028\n",
            "Epoch 20 Loss: -40.626\n",
            "Epoch 21 Loss: -43.336\n",
            "Epoch 22 Loss: -46.163\n",
            "Epoch 23 Loss: -49.114\n",
            "Epoch 24 Loss: -52.191\n",
            "Epoch 25 Loss: -55.401\n",
            "Epoch 26 Loss: -58.745\n",
            "Epoch 27 Loss: -62.228\n",
            "Epoch 28 Loss: -65.853\n",
            "Epoch 29 Loss: -69.621\n",
            "Epoch 30 Loss: -73.536\n",
            "Epoch 31 Loss: -77.597\n",
            "Epoch 32 Loss: -81.806\n",
            "Epoch 33 Loss: -86.164\n",
            "Epoch 34 Loss: -90.671\n",
            "Epoch 35 Loss: -95.328\n",
            "Epoch 36 Loss: -100.134\n",
            "Epoch 37 Loss: -105.089\n",
            "Epoch 38 Loss: -110.194\n",
            "Epoch 39 Loss: -115.448\n",
            "Epoch 40 Loss: -120.851\n",
            "Epoch 41 Loss: -126.403\n",
            "Epoch 42 Loss: -132.104\n",
            "Epoch 43 Loss: -137.955\n",
            "Epoch 44 Loss: -143.954\n",
            "Epoch 45 Loss: -150.104\n",
            "Epoch 46 Loss: -156.403\n",
            "Epoch 47 Loss: -162.852\n",
            "Epoch 48 Loss: -169.451\n",
            "Epoch 49 Loss: -176.200\n",
            "Epoch 50 Loss: -183.100\n",
            "3 most similar words to 'learn': [('learning', 0.43414298), ('!', 0.3971264), ('fascinating', 0.3562515)]\n",
            "3 most similar words to 'deep': [('fascinating', 0.39868173), ('love', 0.34144998), ('to', 0.15409891)]\n",
            "\n",
            "love [ 1.4775798   0.8041824  -2.614521   -3.7062607  -0.62723225 -4.1373773\n",
            " -3.8834667  -2.1617348   1.737296    5.312615  ]\n",
            "fascinating [-1.7544217 -1.167995  -2.144297  -0.7099107 -0.78177   -3.8719144\n",
            "  4.384316  -2.8021953  4.912155  -1.6097721]\n",
            "learning [-2.9400122  -2.2859077   2.8555007   0.25367412  2.9188175  -0.43283987\n",
            " -1.824673   -2.2402902   3.4572022   3.9518719 ]\n",
            "! [-1.1135978  -0.48049936  1.5579455   0.62664634  1.1660824   0.01318888\n",
            " -1.6666498   0.65174854  0.8014188  -0.08017857]\n",
            "It [-2.059316    2.5467362  -3.0649755   3.4732928  -2.2734075   0.79049593\n",
            "  3.7148614  -3.3957176  -2.6177394  -2.345723  ]\n",
            "I [ 2.8204997  -3.687538    3.3412094  -2.1550894  -2.4314075   0.6657372\n",
            " -0.33957657  2.8705516  -3.573452   -4.524783  ]\n",
            "to [-2.6473744 -3.762436  -2.5661087 -3.0196512  5.7184396  5.0815096\n",
            "  4.1962566 -4.4413004 -1.165969  -1.4747025]\n",
            ". [ 0.6558491   0.3500321  -1.1028589   0.710806    0.40526348  0.47484872\n",
            " -0.58851224  1.0177428   2.113282    0.21641515]\n",
            "learn [-2.010258  -1.9150654  5.976407  -1.8308094 -2.7581506 -2.6137593\n",
            " -2.635566  -4.566281   4.4235086 -2.999052 ]\n",
            "is [-2.3327806  -2.249495   -4.3764625  -1.5924634   6.6428776   2.460241\n",
            " -2.4343288   0.14639644 -3.527408   -3.5387223 ]\n",
            "deep [-1.004316   -3.4670644  -0.37852404 -2.6947887  -3.2636898  -3.622468\n",
            "  5.217881   -3.3841105  -2.1899133   5.32098   ]\n"
          ]
        }
      ]
    }
  ]
}